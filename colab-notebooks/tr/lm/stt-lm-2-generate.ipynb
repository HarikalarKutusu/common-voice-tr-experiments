{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"stt-lm-2-generate.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PX19Ree5UBZP"},"source":["# Create a Turkish Language model for STT\n","## Step #2 : Language Model Generation Phase\n","Based on: https://github.com/ftyers/commonvoice-docker/blob/main/lm.sh"]},{"cell_type":"markdown","metadata":{"id":"fMRwVKFCJmtm"},"source":["## Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"V4b9iJI7JZ6e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638609159158,"user_tz":-180,"elapsed":20968,"user":{"displayName":"Dev2 CommonVoice","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12653315035866225755"}},"outputId":"3f9acf19-0c41-459b-e711-ddf37a58ae89"},"source":["# mount your private google drive\n","from google.colab import drive\n","import shutil\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"NAMwVOvtJvZj"},"source":["## Basic Setup"]},{"cell_type":"code","metadata":{"id":"URSGwY5qr3gQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638609237446,"user_tz":-180,"elapsed":62151,"user":{"displayName":"Dev2 CommonVoice","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12653315035866225755"}},"outputId":"00bb0772-505e-4e20-8394-847e8470054a"},"source":["# Install Coqui STT \n","!git clone --depth 1 --branch v1.0.0 https://github.com/coqui-ai/STT.git\n","!cd STT; pip install -U pip wheel setuptools; pip install ."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'STT'...\n","remote: Enumerating objects: 2162, done.\u001b[K\n","remote: Counting objects: 100% (2162/2162), done.\u001b[K\n","remote: Compressing objects: 100% (1358/1358), done.\u001b[K\n","remote: Total 2162 (delta 847), reused 1695 (delta 709), pack-reused 0\u001b[K\n","Receiving objects: 100% (2162/2162), 12.49 MiB | 21.31 MiB/s, done.\n","Resolving deltas: 100% (847/847), done.\n","Note: checking out '27584037f879442fb45f9064dc772dbcb6ba6372'.\n","\n","You are in 'detached HEAD' state. You can look around, make experimental\n","changes and commit them, and you can discard any commits you make in this\n","state without impacting any branches by performing another checkout.\n","\n","If you want to create a new branch to retain commits you create, you may\n","do so (now or later) by using -b with the checkout command again. Example:\n","\n","  git checkout -b <new-branch-name>\n","\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting pip\n","  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n","Collecting setuptools\n","  Downloading setuptools-59.4.0-py3-none-any.whl (952 kB)\n","\u001b[K     |████████████████████████████████| 952 kB 48.4 MB/s \n","\u001b[?25hInstalling collected packages: setuptools, pip\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed pip-21.3.1 setuptools-59.4.0\n","Processing /content/STT\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting attrdict\n","  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.0.0) (0.0.1)\n","Collecting coqpit\n","  Downloading coqpit-0.0.14-py3-none-any.whl (13 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.0.0) (1.19.5)\n","Collecting optuna\n","  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n","     |████████████████████████████████| 308 kB 8.9 MB/s            \n","\u001b[?25hRequirement already satisfied: numba<=0.53.1 in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.0.0) (0.51.2)\n","Collecting opuslib==2.0.0\n","  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.0.0) (1.1.5)\n","Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.0.0) (3.38.0)\n","Collecting pyogg>=0.6.14a1\n","  Downloading PyOgg-0.6.14a1.tar.gz (35 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pyxdg\n","  Downloading pyxdg-0.27-py2.py3-none-any.whl (49 kB)\n","     |████████████████████████████████| 49 kB 6.0 MB/s             \n","\u001b[?25hRequirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.0.0) (0.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.0.0) (2.23.0)\n","Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.0.0) (2.13.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.0.0) (1.15.0)\n","Collecting sox\n","  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.0.0) (0.10.3.post1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from coqui-stt-training==1.0.0) (4.62.3)\n","Collecting coqui_stt_ctcdecoder==1.0.0\n","  Downloading coqui_stt_ctcdecoder-1.0.0-cp37-cp37m-manylinux_2_24_x86_64.whl (2.1 MB)\n","     |████████████████████████████████| 2.1 MB 62.4 MB/s            \n","\u001b[?25hCollecting tensorflow==1.15.4\n","  Downloading tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n","     |████████████████████████████████| 110.5 MB 46 kB/s             \n","\u001b[?25hCollecting numpy\n","  Downloading numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n","     |████████████████████████████████| 14.5 MB 44.2 MB/s            \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (0.2.0)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (1.15.1)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (1.15.0)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (1.42.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (1.13.3)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (0.12.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (1.0.8)\n","Collecting numpy\n","  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n","     |████████████████████████████████| 20.1 MB 1.2 MB/s             \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (3.17.3)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (0.8.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (1.1.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (0.37.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->coqui-stt-training==1.0.0) (3.3.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<=0.53.1->coqui-stt-training==1.0.0) (0.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<=0.53.1->coqui-stt-training==1.0.0) (59.4.0)\n","Requirement already satisfied: scipy>=0.13 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->coqui-stt-training==1.0.0) (1.4.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->coqui-stt-training==1.0.0) (4.6.3)\n","Collecting cliff\n","  Downloading cliff-3.10.0-py3-none-any.whl (80 kB)\n","     |████████████████████████████████| 80 kB 9.4 MB/s             \n","\u001b[?25hCollecting alembic\n","  Downloading alembic-1.7.5-py3-none-any.whl (209 kB)\n","     |████████████████████████████████| 209 kB 63.7 MB/s            \n","\u001b[?25hCollecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna->coqui-stt-training==1.0.0) (3.13)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui-stt-training==1.0.0) (21.3)\n","Collecting colorlog\n","  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->coqui-stt-training==1.0.0) (1.4.27)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->coqui-stt-training==1.0.0) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->coqui-stt-training==1.0.0) (2018.9)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2->coqui-stt-training==1.0.0) (2.5.6)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->coqui-stt-training==1.0.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->coqui-stt-training==1.0.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->coqui-stt-training==1.0.0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->coqui-stt-training==1.0.0) (2.10)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->coqui-stt-training==1.0.0) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->coqui-stt-training==1.0.0) (2.21)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4->coqui-stt-training==1.0.0) (3.1.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna->coqui-stt-training==1.0.0) (3.0.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->coqui-stt-training==1.0.0) (4.8.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /tensorflow-1.15.2/python3.7 (from sqlalchemy>=1.1.0->optuna->coqui-stt-training==1.0.0) (0.4.15)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->coqui-stt-training==1.0.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->coqui-stt-training==1.0.0) (3.3.6)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna->coqui-stt-training==1.0.0) (5.4.0)\n","Collecting Mako\n","  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n","     |████████████████████████████████| 75 kB 4.9 MB/s             \n","\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.8.0-py2.py3-none-any.whl (112 kB)\n","     |████████████████████████████████| 112 kB 79.2 MB/s            \n","\u001b[?25hCollecting autopage>=0.4.0\n","  Downloading autopage-0.4.0-py3-none-any.whl (20 kB)\n","Collecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n","     |████████████████████████████████| 49 kB 5.9 MB/s             \n","\u001b[?25hCollecting cmd2>=1.0.0\n","  Downloading cmd2-2.3.3-py3-none-any.whl (149 kB)\n","     |████████████████████████████████| 149 kB 75.5 MB/s            \n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->coqui-stt-training==1.0.0) (2.4.0)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui-stt-training==1.0.0) (21.2.0)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui-stt-training==1.0.0) (0.2.5)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->coqui-stt-training==1.0.0) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna->coqui-stt-training==1.0.0) (3.6.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.4->coqui-stt-training==1.0.0) (1.5.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna->coqui-stt-training==1.0.0) (2.0.1)\n","Building wheels for collected packages: coqui-stt-training, opuslib, gast, pyogg, pyperclip\n","  Building wheel for coqui-stt-training (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for coqui-stt-training: filename=coqui_stt_training-1.0.0-py3-none-any.whl size=71017 sha256=de9d31e11b7efdda59b08843d81ab010d6eb1d44f0b9bdedafe0672e2588c747\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-knx18du1/wheels/59/0f/67/991d0aff52677d5260e383afececfae694d28b02bfeaacf4ed\n","  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=11008 sha256=dbeb2877d25ddb73f4c344ec3e5bfe644a7cda5457e12a060eb44722a418fe90\n","  Stored in directory: /root/.cache/pip/wheels/e5/ba/d4/0e81231a9797fbb262ae3a54fd761fab850db7f32d94a3283a\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=3c31cddcc51a8af292f51969984e7b0218df3a28295c381d596a494abc0a4e9e\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","  Building wheel for pyogg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyogg: filename=PyOgg-0.6.14a1-py2.py3-none-any.whl size=35330 sha256=7449bf7b32798c201db6eaa561057d3b73d4a67660cbf0790d2376c2679f4f4e\n","  Stored in directory: /root/.cache/pip/wheels/45/aa/6f/5fb54a0a14846e4202945a65937c7f3eb245af5031a141147a\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=0d935b068073137ecfa3d74e620d93b26b78320311b496cd018439beea90e847\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built coqui-stt-training opuslib gast pyogg pyperclip\n","Installing collected packages: pyperclip, pbr, numpy, stevedore, Mako, cmd2, autopage, gast, colorlog, cmaes, cliff, alembic, tensorflow, sox, pyxdg, pyogg, opuslib, optuna, coqui-stt-ctcdecoder, coqpit, attrdict, coqui-stt-training\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 1.15.2\n","    Uninstalling tensorflow-1.15.2:\n","      Successfully uninstalled tensorflow-1.15.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lucid 0.3.10 requires umap-learn, which is not installed.\n","kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.4 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed Mako-1.1.6 alembic-1.7.5 attrdict-2.0.1 autopage-0.4.0 cliff-3.10.0 cmaes-0.8.2 cmd2-2.3.3 colorlog-6.6.0 coqpit-0.0.14 coqui-stt-ctcdecoder-1.0.0 coqui-stt-training-1.0.0 gast-0.2.2 numpy-1.18.5 optuna-2.10.0 opuslib-2.0.0 pbr-5.8.0 pyogg-0.6.14a1 pyperclip-1.8.2 pyxdg-0.27 sox-1.4.1 stevedore-3.5.0 tensorflow-2.7.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FLp19SjmFBLj","executionInfo":{"status":"ok","timestamp":1638615770430,"user_tz":-180,"elapsed":101523,"user":{"displayName":"Dev2 CommonVoice","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12653315035866225755"}},"outputId":"5382b618-ef36-4fe7-e306-4aa795c44f2a"},"source":["# Get KenLM\n","!git clone https://github.com/kpu/kenlm.git && cd kenlm && mkdir build && cd build/ && cmake .. && make -j 4"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'kenlm'...\n","remote: Enumerating objects: 14051, done.\u001b[K\n","remote: Counting objects: 100% (364/364), done.\u001b[K\n","remote: Compressing objects: 100% (296/296), done.\u001b[K\n","remote: Total 14051 (delta 109), reused 121 (delta 55), pack-reused 13687\u001b[K\n","Receiving objects: 100% (14051/14051), 5.76 MiB | 16.24 MiB/s, done.\n","Resolving deltas: 100% (7989/7989), done.\n","-- The C compiler identification is GNU 7.5.0\n","-- The CXX compiler identification is GNU 7.5.0\n","-- Check for working C compiler: /usr/bin/cc\n","-- Check for working C compiler: /usr/bin/cc -- works\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Check for working CXX compiler: /usr/bin/c++\n","-- Check for working CXX compiler: /usr/bin/c++ -- works\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n","-- Looking for pthread.h\n","-- Looking for pthread.h - found\n","-- Looking for pthread_create\n","-- Looking for pthread_create - not found\n","-- Looking for pthread_create in pthreads\n","-- Looking for pthread_create in pthreads - not found\n","-- Looking for pthread_create in pthread\n","-- Looking for pthread_create in pthread - found\n","-- Found Threads: TRUE  \n","-- Boost version: 1.65.1\n","-- Found the following Boost libraries:\n","--   program_options\n","--   system\n","--   thread\n","--   unit_test_framework\n","--   chrono\n","--   date_time\n","--   atomic\n","-- Check if compiler accepts -pthread\n","-- Check if compiler accepts -pthread - yes\n","-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n","-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.6\") \n","-- Looking for BZ2_bzCompressInit\n","-- Looking for BZ2_bzCompressInit - found\n","-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n","-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n","-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n","-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n","-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n","-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n","-- Found LibLZMA: /usr/include (found version \"5.2.2\") \n","-- Looking for clock_gettime in rt\n","-- Looking for clock_gettime in rt - found\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/kenlm/build\n","\u001b[35m\u001b[1mScanning dependencies of target kenlm_util\u001b[0m\n","[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n","[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n","[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n","[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n","[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n","[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n","[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n","[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n","[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n","[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n","[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n","[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n","[ 38%] Built target kenlm_util\n","\u001b[35m\u001b[1mScanning dependencies of target probing_hash_table_benchmark\u001b[0m\n","\u001b[35m\u001b[1mScanning dependencies of target kenlm\u001b[0m\n","\u001b[35m\u001b[1mScanning dependencies of target kenlm_filter\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n","[ 41%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n","[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n","[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n","[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n","[ 52%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n","[ 52%] Built target kenlm_filter\n","[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n","[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n","[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n","[ 62%] Built target probing_hash_table_benchmark\n","[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n","[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n","[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n","[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n","[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n","[ 71%] Built target kenlm\n","\u001b[35m\u001b[1mScanning dependencies of target query\u001b[0m\n","\u001b[35m\u001b[1mScanning dependencies of target build_binary\u001b[0m\n","\u001b[35m\u001b[1mScanning dependencies of target kenlm_benchmark\u001b[0m\n","\u001b[35m\u001b[1mScanning dependencies of target fragment\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n","[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n","[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n","[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n","[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n","[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n","[ 78%] Built target fragment\n","\u001b[35m\u001b[1mScanning dependencies of target kenlm_builder\u001b[0m\n","[ 80%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n","[ 80%] Built target build_binary\n","\u001b[35m\u001b[1mScanning dependencies of target phrase_table_vocab\u001b[0m\n","[ 81%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n","[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n","[ 82%] Built target query\n","\u001b[35m\u001b[1mScanning dependencies of target filter\u001b[0m\n","[ 83%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n","[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n","[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n","[ 86%] Built target phrase_table_vocab\n","[ 87%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n","[ 88%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n","[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n","[ 91%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n","[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n","[ 92%] Built target kenlm_benchmark\n","[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n","[ 93%] Built target filter\n","[ 95%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n","[ 95%] Built target kenlm_builder\n","\u001b[35m\u001b[1mScanning dependencies of target count_ngrams\u001b[0m\n","\u001b[35m\u001b[1mScanning dependencies of target lmplz\u001b[0m\n","[ 96%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n","[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n","[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n","[ 98%] Built target lmplz\n","[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n","[100%] Built target count_ngrams\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jj47NfPTrtmP"},"source":["## Directory Structure"]},{"cell_type":"code","metadata":{"id":"JVNoPhpZWOOh"},"source":["# Copy corpus data from drive\n","!mkdir -p /content/data/tr/lm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cCDUJlnIldHK"},"source":["## Generate Language Model"]},{"cell_type":"code","metadata":{"id":"qI4dsvXgldHK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638620547558,"user_tz":-180,"elapsed":4500779,"user":{"displayName":"Dev2 CommonVoice","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12653315035866225755"}},"outputId":"34cca427-aa1a-4100-8391-a81b9004c4a8"},"source":["!python3 /content/STT/data/lm/generate_lm.py \\\n","  --input_txt /content/drive/MyDrive/cv-datasets/tr/language_model/corpus/corpus.tar.gz \\\n","  --output_dir /content/data/tr/lm/ \\\n","  --top_k 500000 \\\n","  --discount_fallback \\\n","  --kenlm_bins /content/kenlm/build/bin/ \\\n","  --arpa_order 5 \\\n","  --max_arpa_memory \"85%\" \\\n","  --arpa_prune \"0|0|1\" \\\n","  --binary_a_bits 255 \\\n","  --binary_q_bits 8 \\\n","  --binary_type trie"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Converting to lowercase and counting word occurrences ...\n","| |      #                                     | 169045510 Elapsed Time: 0:30:57\n","\n","Saving top 500000 words ...\n","\n","Calculating word statistics ...\n","  Your text file has 686554726 words in total\n","  It has 2229955 unique words\n","  Your top-500000 words are 99.0424 percent of all words\n","  Your most common word \"bir\" occurred 18607643 times\n","  The least common word in your top-k is \"arkadaşımlayken\" with 17 times\n","  The first word with 18 occurrences is \"isirika\" at place 486792\n","\n","Creating ARPA file ...\n","=== 1/5 Counting and sorting n-grams ===\n","Reading /content/data/tr/lm/lower.txt.gz\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","tcmalloc: large alloc 2038890496 bytes == 0x55b91bb62000 @  0x7fea04e931e7 0x55b91ac577a2 0x55b91abf251e 0x55b91abd12eb 0x55b91abbd066 0x7fea0302cbf7 0x55b91abbebaa\n","tcmalloc: large alloc 9514811392 bytes == 0x55b9953d2000 @  0x7fea04e931e7 0x55b91ac577a2 0x55b91ac467ca 0x55b91ac47208 0x55b91abd1308 0x55b91abbd066 0x7fea0302cbf7 0x55b91abbebaa\n","****************************************************************************************************\n","Unigram tokens 686554734 types 2229961\n","=== 2/5 Calculating and sorting adjusted counts ===\n","Chain sizes: 1:26759532 2:1113939584 3:2088636800 4:3341818624 5:4873485824\n","tcmalloc: large alloc 4873486336 bytes == 0x55b91ba54000 @  0x7fea04e931e7 0x55b91ac577a2 0x55b91ac467ca 0x55b91ac47208 0x55b91abd18d7 0x55b91abbd066 0x7fea0302cbf7 0x55b91abbebaa\n","tcmalloc: large alloc 2088640512 bytes == 0x55ba821f0000 @  0x7fea04e931e7 0x55b91ac577a2 0x55b91ac467ca 0x55b91ac47208 0x55b91abd1cdd 0x55b91abbd066 0x7fea0302cbf7 0x55b91abbebaa\n","tcmalloc: large alloc 3341819904 bytes == 0x55bafe9d2000 @  0x7fea04e931e7 0x55b91ac577a2 0x55b91ac467ca 0x55b91ac47208 0x55b91abd1cdd 0x55b91abbd066 0x7fea0302cbf7 0x55b91abbebaa\n","Statistics:\n","1 2229961 D1=0.657663 D2=1.00368 D3+=1.34388\n","2 47419926 D1=0.773528 D2=1.11258 D3+=1.35614\n","3 71503793/122094645 D1=0.839795 D2=1.05748 D3+=1.19349\n","4 84852585/153979140 D1=0.857754 D2=0.901156 D3+=1.10661\n","5 74128618/140037785 D1=0.503525 D2=1.22432 D3+=1.91325\n","Memory estimate for binary LM:\n","type      MB\n","probing 5991 assuming -p 1.5\n","probing 7166 assuming -r models -p 1.5\n","trie    3240 without quantization\n","trie    1895 assuming -q 8 -b 8 quantization \n","trie    2756 assuming -a 22 array pointer compression\n","trie    1411 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n","=== 3/5 Calculating and sorting initial probabilities ===\n","tcmalloc: large alloc 2441895936 bytes == 0x55b91ba54000 @  0x7fea04e931e7 0x55b91ac577a2 0x55b91abdc352 0x55b91abdd2ed 0x7fea04173bcd 0x7fea03d426db 0x7fea0312c71f\n","tcmalloc: large alloc 3695501312 bytes == 0x55b91ba54000 @  0x7fea04e931e7 0x55b91ac577a2 0x55b91abdc352 0x55b91abdd2ed 0x7fea04173bcd 0x7fea03d426db 0x7fea0312c71f\n","tcmalloc: large alloc 3921059840 bytes == 0x55b91ba54000 @  0x7fea04e931e7 0x55b91ac577a2 0x55b91abdc352 0x55b91abdd2ed 0x7fea04173bcd 0x7fea03d426db 0x7fea0312c71f\n","Chain sizes: 1:26759532 2:758718816 3:1430075860 4:2036462040 5:2075601304\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","############################************************################################################\n","=== 4/5 Calculating and writing order-interpolated probabilities ===\n","Chain sizes: 1:26759532 2:758718816 3:1430075860 4:2036462040 5:2075601304\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","####################################################################################################\n","=== 5/5 Writing ARPA model ===\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Name:lmplz\tVmPeak:11865188 kB\tVmRSS:6213080 kB\tRSSMax:10977808 kB\tuser:1019.11\tsys:145.795\tCPU:1164.9\treal:1807.86\n","\n","Filtering ARPA file using vocabulary of top-k words ...\n","Reading /content/data/tr/lm/lm.arpa\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","\n","Building lm.binary ...\n","Reading /content/data/tr/lm/lm_filtered.arpa\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","tcmalloc: large alloc 1945493504 bytes == 0x557bd32f2000 @  0x7f7ec81ca1e7 0x557bd161e99e 0x557bd1618070 0x557bd15f1800 0x557bd15f58e4 0x557bd15e7d34 0x7f7ec6a0ebf7 0x557bd15e86aa\n","****************************************************************************************************\n","Identifying n-grams omitted by SRI\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Quantizing\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Writing trie\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","SUCCESS\n"]}]},{"cell_type":"markdown","metadata":{"id":"QOxZnGbfldHL"},"source":["## Save Results"]},{"cell_type":"code","metadata":{"id":"2j9QZb0VldHL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638620847225,"user_tz":-180,"elapsed":293,"user":{"displayName":"Dev2 CommonVoice","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12653315035866225755"}},"outputId":"ce072962-ec6d-4097-e74a-dddd6e210c39"},"source":["!ls -al /content/data/tr/lm/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 1228736\n","drwxr-xr-x 2 root root       4096 Dec  4 12:22 .\n","drwxr-xr-x 3 root root       4096 Dec  4 09:40 ..\n","-rw-r--r-- 1 root root 1252410121 Dec  4 12:22 lm.binary\n","-rw-r--r-- 1 root root    5796451 Dec  4 11:38 vocab-500000.txt\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"eaDeh672E5J2","executionInfo":{"status":"ok","timestamp":1638620975135,"user_tz":-180,"elapsed":97048,"user":{"displayName":"Dev2 CommonVoice","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12653315035866225755"}},"outputId":"ccae3ee5-8460-4fd2-bee6-42a186c26421"},"source":["# Compress\n","#!tar czf /content/data/lm.tar.gz /content/data/tr/lm\n","# Copy file to Google Drive.\n","shutil.move(\"/content/data/tr/lm/lm.binary\", \"/content/drive/MyDrive/cv-datasets/tr/language_model/lm\")\n","shutil.move(\"/content/data/tr/lm/vocab-500000.txt\", \"/content/drive/MyDrive/cv-datasets/tr/language_model/lm\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tar: Removing leading `/' from member names\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/cv-datasets/tr/lm.tar.gz'"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"BpA5FDBYL-uO"},"source":["# Flush disk to Google Drive\n","drive.flush_and_unmount()"],"execution_count":null,"outputs":[]}]}